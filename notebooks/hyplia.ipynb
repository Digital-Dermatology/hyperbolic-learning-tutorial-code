{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Hyperbolic Learning in Action: Practice\n",
    "\n",
    "In this notebook, we are going to train, evaluate, and compare three Convolutional Neural Networks (CNNs):\n",
    "\n",
    "1. an ordinary, fully Euclidean one;\n",
    "2. one with the last layer in hyperbolic space;\n",
    "3. a fully hyperbolic network.\n",
    "\n",
    "We will use:\n",
    "\n",
    "- the CIFAR-10 and CIFAR-100 datasets, whereas the first is chosen for its simplicity and the second because it exhibits *hierarchical* structure;\n",
    "- the hyperbolic learning library `HypLL` for the hyperbolic layers, due to its ease of use.\n",
    "\n",
    "We will visualize data representations in the Euclidean and hyperbolic space.\n",
    "\n",
    "## Setup\n",
    "\n",
    "**If you are on Colab or Kaggle, get GPU acceleration**\n",
    "- Colab:\n",
    "    1. Click on the dropdown arrow on the right of the menu bar above the notebook, next to \"Connect\".\n",
    "    2. Select \"Change runtime type\".\n",
    "    3. Choose \"T4 GPU\" under \"Hardware accelerator\".\n",
    "- Kaggle:\n",
    "    1. Expand the section \"Session options\" on the right menu sidebar.\n",
    "    2. Select \"GPU P100\" under \"Accelerator\".\n",
    "\n",
    "### Environment\n",
    "\n",
    "Check if the notebook is already in the code repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path_parts = os.getcwd().split(os.sep)\n",
    "repository_path = \"\"\n",
    "try:\n",
    "    repository_index = path_parts.index(\"hyperbolic-learning-tutorial-code\")\n",
    "    repository_path = os.sep.join(path_parts[: repository_index + 1])\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Get the repository if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if repository_path == \"\":\n",
    "    !git clone https://github.com/Digital-Dermatology/hyperbolic-learning-tutorial-code.git\n",
    "    %cd hyperbolic-learning-tutorial-code\n",
    "    repository_path = \"hyperbolic-learning-tutorial-code\"\n",
    "else:\n",
    "    %cd {repository_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Install requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Add the project's root to the Python path for custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(repository_path, \"src\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Set the `torch` device and seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.utils.torch_utils import get_available_device, set_seeds\n",
    "\n",
    "device = torch.device(get_available_device())\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Get the datasets.\n",
    "\n",
    "Since this is a demonstration, and it does not use hyperparameter tuning, it is ok to work only with one split for training and one for evaluation, i.e. testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=(0.5, 0.5, 0.5),\n",
    "            std=(0.5, 0.5, 0.5),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "classes = train_dataset.classes\n",
    "assert test_dataset.classes == classes\n",
    "num_classes = len(classes)\n",
    "print(f\"Classes in the dataset: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Prepare the data loaders.\n",
    "\n",
    "The batch size and the number of workers may be adjusted as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_workers = 0\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Euclidean Network\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Start with a simple Euclidean convolutional network.\n",
    "\n",
    "To compare with hyperbolic networks without too much pain:\n",
    "\n",
    "- it has no batch normalization nor skip connections;\n",
    "- fully connected layers are used at the end instead of e.g. global pooling;\n",
    "- no transfer learning is used.\n",
    "\n",
    "Right before classification, reduce the input dimension to 2 to enable embedding visualization.\n",
    "This will lead to poor performance in Euclidean space, but it will be less of a problem for hyperbolic networks.\n",
    "The constraint can be relaxed if a dimensionality reduction method such as PCA, t-SNE, or UMAP is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d, Flatten, Linear, MaxPool2d, ReLU, Sequential\n",
    "\n",
    "last_channels = 3\n",
    "conv_channels = (32, 64, 128)\n",
    "fc_channels = (128, 32, 2)\n",
    "image_size = (32, 32)\n",
    "pool_kernel_size = 2\n",
    "pool_stride = 2\n",
    "conv_kernel_size = 3\n",
    "\n",
    "pool = MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride)\n",
    "activation = ReLU()\n",
    "current_image_size = torch.tensor(image_size)\n",
    "layers = []\n",
    "for channels in conv_channels:\n",
    "    layers.append(\n",
    "        Conv2d(in_channels=last_channels, out_channels=channels, kernel_size=3)\n",
    "    )\n",
    "    current_image_size -= conv_kernel_size - 1\n",
    "    layers.append(activation)\n",
    "    layers.append(pool)\n",
    "    current_image_size //= pool_stride\n",
    "    last_channels = channels\n",
    "layers.append(Flatten())\n",
    "last_channels *= current_image_size.prod()\n",
    "for channels in fc_channels:\n",
    "    layers.append(\n",
    "        Linear(in_features=last_channels, out_features=channels)\n",
    "    )\n",
    "    layers.append(activation)\n",
    "    last_channels = channels\n",
    "layers = layers[:-1]  # remove the last activation\n",
    "layers.append(Linear(in_features=last_channels, out_features=len(classes)))\n",
    "euclidean_network = Sequential(*layers)\n",
    "euclidean_network = euclidean_network.to(device)\n",
    "euclidean_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for data, labels in test_dataloader:\n",
    "        outputs = euclidean_network(data.to(device))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(euclidean_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Define the metrics for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassMatthewsCorrCoef\n",
    "\n",
    "metrics = MetricCollection(\n",
    "    [\n",
    "        MulticlassAccuracy(num_classes=num_classes),\n",
    "        MulticlassMatthewsCorrCoef(num_classes=num_classes),\n",
    "    ]\n",
    ")\n",
    "metrics = metrics.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Evaluate before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(metrics: MetricCollection, prefix: str = \"\") -> None:\n",
    "    print(\n",
    "        prefix,\n",
    "        {k.replace(\"Multiclass\", \"\"): v.item() for k, v in metrics.compute().items()},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.reset()\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_dataloader:\n",
    "        outputs = euclidean_network(data.to(device))\n",
    "        metrics(outputs, labels.to(device))\n",
    "print_metrics(metrics, \"Metrics before training:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion.to(device)\n",
    "optimizer = Adam(euclidean_network.parameters(), lr=1e-3)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} of {num_epochs}\")\n",
    "    metrics.reset()\n",
    "    for data, labels in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = euclidean_network(data.to(device))\n",
    "        labels = labels.to(device)\n",
    "        metrics(outputs, labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print_metrics(metrics, \"Train: \")\n",
    "    metrics.reset()\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_dataloader:\n",
    "            outputs = euclidean_network(data.to(device))\n",
    "            metrics(outputs, labels.to(device))\n",
    "    print_metrics(metrics, \"Test: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Get the embeddings to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, predictions, labels = [], [], []\n",
    "with torch.no_grad():\n",
    "    for data, labels_batch in test_dataloader:\n",
    "        embeddings_batch = euclidean_network[:-1](data.to(device))\n",
    "        predictions_batch = euclidean_network[-1](embeddings_batch).argmax(dim=-1)\n",
    "        embeddings.append(embeddings_batch)\n",
    "        predictions.append(predictions_batch)\n",
    "        labels.append(labels_batch)\n",
    "embeddings = torch.cat(embeddings, dim=0)\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "labels = torch.cat(labels, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Save the plot to HTML to avoid overloading the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "df = pd.DataFrame(embeddings.cpu().numpy())\n",
    "df[\"prediction\"] = predictions.cpu().numpy()\n",
    "df[\"label\"] = [classes[i] for i in labels.cpu().numpy()]\n",
    "fig = px.scatter(data_frame=df, x=0, y=1, color=\"label\")\n",
    "fig.write_html(\"euclidean.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Last hyperbolic layer\n",
    "\n",
    "Now it is time to roll up sleeves. Enjoy hacking!\n",
    "\n",
    "1. Define the hyperbolic manifold using [`hypll.manifolds.poincare_ball.PoincareBall`](https://hyperbolic-learning-library.readthedocs.io/en/latest/_autosummary/hypll.manifolds.poincare_ball.manifold.html) with a trainable curvature parameter [`hypll.manifolds.poincare_ball.Curvature`](https://hyperbolic-learning-library.readthedocs.io/en/latest/_autosummary/hypll.manifolds.poincare_ball.curvature.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifold = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "2. Starting with the Euclidean network, just before classification, lift the representation to hyperbolic space by constructing a [`hypll.tensors.TangentTensor`](https://hyperbolic-learning-library.readthedocs.io/en/latest/_autosummary/hypll.tensors.tangent_tensor.html) and using `PoincareBall`'s exponential map.\n",
    "    - Hint: you may also use the convenience layer [`src.layers.to_manifold.ToManifold`](https://github.com/Digital-Dermatology/hyperbolic-learning-tutorial-code/blob/main/src/layers/to_manifold.py) or take inspiration from it.\n",
    "3. Obtain the logits by replacing the linear classification layer of the Euclidean network with the calculation of the distances from (learned) hyperbolic hyperplanes.\n",
    "    - This operation, known as Hyperbolic Multinomial Logistic Regression, is implemented in [`src.layers.hmlr.HMLR`](https://github.com/Digital-Dermatology/hyperbolic-learning-tutorial-code/blob/main/src/layers/hmlr.py), feel free to use it directly or as a guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hyperbolic_network = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "4. Replace the Adam optimizer with Riemannian Adam from [`hypll.optim.RiemannianAdam`](https://hyperbolic-learning-library.readthedocs.io/en/latest/_autosummary/hypll.optim.adam.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "riemannian_optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "5. Train the network for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "6. Visualize the embeddings with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, predictions, labels = [], [], []\n",
    "with torch.no_grad():\n",
    "    for data, labels_batch in test_dataloader:\n",
    "        embeddings_batch = ...\n",
    "        predictions_batch = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "7. Compare the training time, final performance, and representations with the euclidean ones!\n",
    "\n",
    "## Fully hyperbolic network\n",
    "\n",
    "Exercise 2:\n",
    "\n",
    "1. Define the hyperbolic manifold as in Exercise 1.\n",
    "2. Immediately after getting data from the `DataLoader`, lift it to the `PoincareBall` as in the previous exercise.\n",
    "3. Build a fully hyperbolic backbone using the layers `HLinear`, `HConv2D`, `HPool2D`, and `HReLU` from `hypll.nn`.\n",
    "4. Add the classification layer at the end using `src.layers.hmlr.HMLR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Optional: CIFAR-100\n",
    "\n",
    "If you got this far, well done!!\n",
    "\n",
    "You should repeat the exercise with CIFAR-100, which has a more hierarchical structure, to see the benefits of hyperbolic learning for real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
